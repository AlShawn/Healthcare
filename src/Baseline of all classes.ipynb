{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#PHM2017-Dataset-Basic-Exploration\" data-toc-modified-id=\"PHM2017-Dataset-Basic-Exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>PHM2017 Dataset Basic Exploration</a></span></li><li><span><a href=\"#clean-the-dataset\" data-toc-modified-id=\"clean-the-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>clean the dataset</a></span></li><li><span><a href=\"#load-word-embeddings-from-the-Glove\" data-toc-modified-id=\"load-word-embeddings-from-the-Glove-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>load word embeddings from the Glove</a></span></li><li><span><a href=\"#prepare-dataset\" data-toc-modified-id=\"prepare-dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>prepare dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#change-df_to_evaluate-to-eval-different-dataset\" data-toc-modified-id=\"change-df_to_evaluate-to-eval-different-dataset-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>change df_to_evaluate to eval different dataset</a></span></li><li><span><a href=\"#functions-to-be-used\" data-toc-modified-id=\"functions-to-be-used-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>functions to be used</a></span></li></ul></li><li><span><a href=\"#baseline-experiment\" data-toc-modified-id=\"baseline-experiment-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>baseline experiment</a></span><ul class=\"toc-item\"><li><span><a href=\"#MLP-keras-version\" data-toc-modified-id=\"MLP-keras-version-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>MLP-keras version</a></span></li><li><span><a href=\"#bi-lstm\" data-toc-modified-id=\"bi-lstm-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>bi-lstm</a></span></li><li><span><a href=\"#GRU+glove\" data-toc-modified-id=\"GRU+glove-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>GRU+glove</a></span></li><li><span><a href=\"#LSTM+glove\" data-toc-modified-id=\"LSTM+glove-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>LSTM+glove</a></span></li><li><span><a href=\"#CNN+glove\" data-toc-modified-id=\"CNN+glove-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>CNN+glove</a></span></li><li><span><a href=\"#char-embedding\" data-toc-modified-id=\"char-embedding-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>char-embedding</a></span><ul class=\"toc-item\"><li><span><a href=\"#cnn-char-model-building\" data-toc-modified-id=\"cnn-char-model-building-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>cnn-char model building</a></span></li><li><span><a href=\"#lstm-(integrate-cnn-char)-model\" data-toc-modified-id=\"lstm-(integrate-cnn-char)-model-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>lstm (integrate cnn-char) model</a></span></li><li><span><a href=\"#lstm-(integrate-rnn-char)-model\" data-toc-modified-id=\"lstm-(integrate-rnn-char)-model-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>lstm (integrate rnn-char) model</a></span></li></ul></li></ul></li><li><span><a href=\"#result-table\" data-toc-modified-id=\"result-table-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>result table</a></span><ul class=\"toc-item\"><li><span><a href=\"#old-result-of-binary-version\" data-toc-modified-id=\"old-result-of-binary-version-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>old result of binary version</a></span><ul class=\"toc-item\"><li><span><a href=\"#f1-score-of-positive-class\" data-toc-modified-id=\"f1-score-of-positive-class-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>f1 score of positive class</a></span></li><li><span><a href=\"#macro-average-of-f1-score\" data-toc-modified-id=\"macro-average-of-f1-score-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>macro average of f1 score</a></span></li></ul></li><li><span><a href=\"#new-result-of-4-classes\" data-toc-modified-id=\"new-result-of-4-classes-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>new result of 4 classes</a></span></li></ul></li><li><span><a href=\"#some-notes-in-training:\" data-toc-modified-id=\"some-notes-in-training:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>some notes in training:</a></span></li><li><span><a href=\"#conclusion\" data-toc-modified-id=\"conclusion-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHM2017 Dataset Basic Exploration\n",
    "\n",
    "for more detailed exploration, please refer to PHM2017.nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5288, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>symptom</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811644183937159168</td>\n",
       "      <td>Had a sleep as I went for 24hrs with no sleep....</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811644622904721408</td>\n",
       "      <td>Owning a cat can reduce the risk of a stroke a...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>811644687899652100</td>\n",
       "      <td>phew. U nearly gave me a heart attack then</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>811644691619991552</td>\n",
       "      <td>@emmabennettx a cheeky reply and you'll give t...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>811644742148702208</td>\n",
       "      <td>A linesman's whistle induced heart attack and ...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet  \\\n",
       "0  811644183937159168  Had a sleep as I went for 24hrs with no sleep....   \n",
       "1  811644622904721408  Owning a cat can reduce the risk of a stroke a...   \n",
       "2  811644687899652100         phew. U nearly gave me a heart attack then   \n",
       "3  811644691619991552  @emmabennettx a cheeky reply and you'll give t...   \n",
       "4  811644742148702208  A linesman's whistle induced heart attack and ...   \n",
       "\n",
       "        symptom  label  \n",
       "0  heart attack      1  \n",
       "1  heart attack      1  \n",
       "2  heart attack      0  \n",
       "3  heart attack      0  \n",
       "4  heart attack      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('../clean_data/PHM2017/PHM2017.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>symptom</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>811644183937159168</td>\n",
       "      <td>sleep went 24hrs sleep esophogus sore yeah giv...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>811644622904721408</td>\n",
       "      <td>own cat reduc risk stroke heart attack third</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>811644687899652100</td>\n",
       "      <td>phew near gave heart attack</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>811644691619991552</td>\n",
       "      <td>emmabennettx cheeki repli give poor bastard so...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>811644742148702208</td>\n",
       "      <td>linesman whistl induc heart attack nice video ...</td>\n",
       "      <td>heart attack</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                              tweet  \\\n",
       "0  811644183937159168  sleep went 24hrs sleep esophogus sore yeah giv...   \n",
       "1  811644622904721408       own cat reduc risk stroke heart attack third   \n",
       "2  811644687899652100                        phew near gave heart attack   \n",
       "3  811644691619991552  emmabennettx cheeki repli give poor bastard so...   \n",
       "4  811644742148702208  linesman whistl induc heart attack nice video ...   \n",
       "\n",
       "        symptom  label  \n",
       "0  heart attack      1  \n",
       "1  heart attack      1  \n",
       "2  heart attack      0  \n",
       "3  heart attack      0  \n",
       "4  heart attack      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "    \n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    ## Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    ## Stemming\n",
    "    text = text.split()\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    return text\n",
    "# apply the above function to df['text']\n",
    "df['tweet'] = df['tweet'].map(lambda x: clean_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length: 87\n"
     ]
    }
   ],
   "source": [
    "m=0\n",
    "for s in df['tweet'].tolist():\n",
    "    if(len(s.split())>m):\n",
    "#         print(s.split())\n",
    "        m=len(s.split())\n",
    "print('max sentence length:',m)\n",
    "\n",
    "### Create sequence\n",
    "vocabulary_size = 11401\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(df['tweet'])\n",
    "sequences = tokenizer.texts_to_sequences(df['tweet'])\n",
    "data = pad_sequences(sequences, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5288, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load word embeddings from the Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('D:/Datasets/glove.6B.300d.txt','r',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alzheimer=df[df['symptom']=='alzheimer'][['tweet','label']]\n",
    "df_cancer=df[df['symptom']=='cancer'][['tweet','label']]\n",
    "df_depression=df[df['symptom']=='depression'][['tweet','label']]\n",
    "df_heart_attack=df[df['symptom']=='heart attack'][['tweet','label']]\n",
    "df_parkinson=df[df['symptom']=='parkinson'][['tweet','label']]\n",
    "df_stroke=df[df['symptom']=='stroke'][['tweet','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## change df_to_evaluate to eval different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change df_to_evaluate to evaluate different types\n",
    "\n",
    "df_to_evaluate=df_stroke\n",
    "\n",
    "stroke_train=data[df_to_evaluate.index]\n",
    "stroke_label=df_to_evaluate.label.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MLP+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890, 300)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=[]\n",
    "for t in df_to_evaluate.tweet.tolist():\n",
    "    dataset.append(t.split())\n",
    "    \n",
    "EMBEDDING_DIM=300\n",
    "data_emb=np.zeros((len(dataset),EMBEDDING_DIM))\n",
    "for i in range(len(dataset)):\n",
    "    each_emb=np.zeros((len(dataset[i]),EMBEDDING_DIM))\n",
    "    for j,w in enumerate(dataset[i]):\n",
    "        if(w in embeddings_index):\n",
    "            each_emb[j]=embeddings_index[w]\n",
    "#             break\n",
    "    data_emb[i]=np.mean(each_emb, axis=0)        \n",
    "        \n",
    "#     nonzero_inds=np.nonzero(dataset[i].flatten())\n",
    "#     review_emb=embedding_matrix[nonzero_inds]\n",
    "#     data_emb[i]=np.mean(review_emb, axis=0)\n",
    "data_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# hidden_neurons=256\n",
    "    \n",
    "# classifier = MLPClassifier(hidden_layer_sizes=(hidden_neurons,hidden_neurons), max_iter=1000, alpha=0.001,\n",
    "#                      solver='adam', verbose=0,  random_state=0)\n",
    "\n",
    "# print(\"start training..\")\n",
    "# # classifier.fit(X_train_emb,y_train)\n",
    "# scoring = ['precision_macro', 'recall_macro','f1_macro']\n",
    "# scores = cross_validate(classifier, data_emb, df_to_evaluate.label, \n",
    "# #                         print evaluation metric for each class\n",
    "# #                          scoring=make_scorer(classification_report_with_accuracy_score),\n",
    "#                          scoring=scoring,\n",
    "#                          cv=5)\n",
    "# print(scores)\n",
    "# print('ave_test_f1_macro:',scores['test_f1_macro'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-1c8b218df798>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m X_train, X_val_test, y_train, y_val_test = train_test_split(data_emb, stroke_label,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                                     \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstroke_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                     \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_emb' is not defined"
     ]
    }
   ],
   "source": [
    "# old result\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(data_emb, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "y_train_cat= np_utils.to_categorical(y_train)\n",
    "y_test_cat= np_utils.to_categorical(y_test)\n",
    "y_val_cat= np_utils.to_categorical(y_val)\n",
    "\n",
    "print(X_train.shape,X_val_test.shape,X_val.shape,X_test.shape)\n",
    "# classifier.fit(X_train,y_train)\n",
    "\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# # y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "# y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP-keras version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_3_input to have shape (300,) but got array with shape (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a5f1a0b6e618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel_mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_cat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_3_input to have shape (300,) but got array with shape (100,)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "model_mlp = Sequential([\n",
    "    Dense(128, input_shape=(300,)),\n",
    "    Activation('relu'),\n",
    "    Dense(4),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model_mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "# set class_weight for imbalanced classes\n",
    "# class_weight={0:1,1:1,2:10,3:10}\n",
    "class_weight={0:1,1:1,2:10,3:10}\n",
    "\n",
    "\n",
    "model_mlp.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs = 50,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        98\n",
      "           1       0.77      0.70      0.73        53\n",
      "           2       0.64      0.70      0.67        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       178\n",
      "   macro avg       0.55      0.56      0.56       178\n",
      "weighted avg       0.76      0.76      0.76       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_prob = model_mlp.predict(X_test)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bi-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(stroke_train, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "y_train_cat= np_utils.to_categorical(y_train)\n",
    "y_test_cat= np_utils.to_categorical(y_test)\n",
    "y_val_cat= np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 100), (178, 100), (534, 4), (178, 4))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train_cat.shape,y_test_cat.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/100\n",
      "534/534 [==============================] - 4s 8ms/step - loss: 2.9268 - acc: 0.1292 - f1_m: 0.0480 - precision_m: 0.1432 - recall_m: 0.0337 - val_loss: 1.6399 - val_acc: 0.1348 - val_f1_m: 0.1168 - val_precision_m: 0.1392 - val_recall_m: 0.1011\n",
      "Epoch 2/100\n",
      "534/534 [==============================] - 2s 3ms/step - loss: 2.5739 - acc: 0.1517 - f1_m: 0.1120 - precision_m: 0.2011 - recall_m: 0.0805 - val_loss: 1.4324 - val_acc: 0.1910 - val_f1_m: 0.1169 - val_precision_m: 0.1706 - val_recall_m: 0.0899\n",
      "Epoch 3/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 2.4103 - acc: 0.2060 - f1_m: 0.1448 - precision_m: 0.2280 - recall_m: 0.1067 - val_loss: 1.3827 - val_acc: 0.2528 - val_f1_m: 0.1718 - val_precision_m: 0.2404 - val_recall_m: 0.1348\n",
      "Epoch 4/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 2.1827 - acc: 0.3596 - f1_m: 0.2311 - precision_m: 0.3743 - recall_m: 0.1704 - val_loss: 1.1368 - val_acc: 0.5618 - val_f1_m: 0.3929 - val_precision_m: 0.5413 - val_recall_m: 0.3090\n",
      "Epoch 5/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 1.9235 - acc: 0.6030 - f1_m: 0.4335 - precision_m: 0.6609 - recall_m: 0.3258 - val_loss: 0.9967 - val_acc: 0.6348 - val_f1_m: 0.4975 - val_precision_m: 0.6896 - val_recall_m: 0.3933\n",
      "Epoch 6/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 1.7085 - acc: 0.6610 - f1_m: 0.5475 - precision_m: 0.7408 - recall_m: 0.4363 - val_loss: 1.2071 - val_acc: 0.5449 - val_f1_m: 0.4669 - val_precision_m: 0.5564 - val_recall_m: 0.4045\n",
      "Epoch 7/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 1.4766 - acc: 0.7004 - f1_m: 0.6655 - precision_m: 0.7775 - recall_m: 0.5843 - val_loss: 1.0497 - val_acc: 0.6461 - val_f1_m: 0.6151 - val_precision_m: 0.6901 - val_recall_m: 0.5562\n",
      "Epoch 8/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 1.3564 - acc: 0.7172 - f1_m: 0.6705 - precision_m: 0.7805 - recall_m: 0.5899 - val_loss: 0.8653 - val_acc: 0.7360 - val_f1_m: 0.7216 - val_precision_m: 0.7696 - val_recall_m: 0.6798\n",
      "Epoch 9/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 1.1840 - acc: 0.7903 - f1_m: 0.7566 - precision_m: 0.8472 - recall_m: 0.6854 - val_loss: 1.1588 - val_acc: 0.6236 - val_f1_m: 0.6021 - val_precision_m: 0.6349 - val_recall_m: 0.5730\n",
      "Epoch 10/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 1.1250 - acc: 0.7659 - f1_m: 0.7440 - precision_m: 0.8160 - recall_m: 0.6854 - val_loss: 0.8509 - val_acc: 0.6910 - val_f1_m: 0.6807 - val_precision_m: 0.7131 - val_recall_m: 0.6517\n",
      "Epoch 11/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 1.1095 - acc: 0.7472 - f1_m: 0.7307 - precision_m: 0.7965 - recall_m: 0.6760 - val_loss: 0.8080 - val_acc: 0.7191 - val_f1_m: 0.7136 - val_precision_m: 0.7601 - val_recall_m: 0.6742\n",
      "Epoch 12/100\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.9705 - acc: 0.7659 - f1_m: 0.7396 - precision_m: 0.8019 - recall_m: 0.6891 - val_loss: 0.7644 - val_acc: 0.7528 - val_f1_m: 0.7449 - val_precision_m: 0.7877 - val_recall_m: 0.7079\n",
      "Epoch 13/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.9137 - acc: 0.8034 - f1_m: 0.7902 - precision_m: 0.8456 - recall_m: 0.7434 - val_loss: 0.7736 - val_acc: 0.7809 - val_f1_m: 0.7510 - val_precision_m: 0.7735 - val_recall_m: 0.7303\n",
      "Epoch 14/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.7854 - acc: 0.8371 - f1_m: 0.8250 - precision_m: 0.8736 - recall_m: 0.7828 - val_loss: 0.8682 - val_acc: 0.7416 - val_f1_m: 0.7257 - val_precision_m: 0.7519 - val_recall_m: 0.7022\n",
      "Epoch 15/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.7169 - acc: 0.8577 - f1_m: 0.8525 - precision_m: 0.8931 - recall_m: 0.8165 - val_loss: 0.8222 - val_acc: 0.7360 - val_f1_m: 0.7400 - val_precision_m: 0.7692 - val_recall_m: 0.7135\n",
      "Epoch 16/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.7335 - acc: 0.8390 - f1_m: 0.8355 - precision_m: 0.8708 - recall_m: 0.8034 - val_loss: 0.8206 - val_acc: 0.7472 - val_f1_m: 0.7491 - val_precision_m: 0.7698 - val_recall_m: 0.7303\n",
      "Epoch 17/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.6746 - acc: 0.8446 - f1_m: 0.8467 - precision_m: 0.8717 - recall_m: 0.8240 - val_loss: 0.8176 - val_acc: 0.7472 - val_f1_m: 0.7572 - val_precision_m: 0.7809 - val_recall_m: 0.7360\n",
      "Epoch 18/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.6291 - acc: 0.8352 - f1_m: 0.8415 - precision_m: 0.8690 - recall_m: 0.8165 - val_loss: 0.8663 - val_acc: 0.7753 - val_f1_m: 0.7737 - val_precision_m: 0.7907 - val_recall_m: 0.7584\n",
      "Epoch 19/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.5891 - acc: 0.8371 - f1_m: 0.8442 - precision_m: 0.8704 - recall_m: 0.8202 - val_loss: 0.8660 - val_acc: 0.6966 - val_f1_m: 0.6854 - val_precision_m: 0.7107 - val_recall_m: 0.6629\n",
      "Epoch 20/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.6386 - acc: 0.8839 - f1_m: 0.8803 - precision_m: 0.9029 - recall_m: 0.8596 - val_loss: 1.0475 - val_acc: 0.6798 - val_f1_m: 0.6709 - val_precision_m: 0.6855 - val_recall_m: 0.6573\n",
      "Epoch 21/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.6891 - acc: 0.8333 - f1_m: 0.8340 - precision_m: 0.8527 - recall_m: 0.8165 - val_loss: 0.8606 - val_acc: 0.6461 - val_f1_m: 0.6781 - val_precision_m: 0.7375 - val_recall_m: 0.6292\n",
      "Epoch 22/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.7236 - acc: 0.7996 - f1_m: 0.7814 - precision_m: 0.8248 - recall_m: 0.7434 - val_loss: 0.7202 - val_acc: 0.7753 - val_f1_m: 0.7755 - val_precision_m: 0.7936 - val_recall_m: 0.7584\n",
      "Epoch 23/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.5912 - acc: 0.8652 - f1_m: 0.8626 - precision_m: 0.8839 - recall_m: 0.8427 - val_loss: 0.8859 - val_acc: 0.7191 - val_f1_m: 0.7153 - val_precision_m: 0.7554 - val_recall_m: 0.6798\n",
      "Epoch 24/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.4821 - acc: 0.8839 - f1_m: 0.8881 - precision_m: 0.9130 - recall_m: 0.8652 - val_loss: 0.7992 - val_acc: 0.7809 - val_f1_m: 0.7756 - val_precision_m: 0.7939 - val_recall_m: 0.7584\n",
      "Epoch 25/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.4194 - acc: 0.9026 - f1_m: 0.8977 - precision_m: 0.9167 - recall_m: 0.8801 - val_loss: 0.8084 - val_acc: 0.7528 - val_f1_m: 0.7533 - val_precision_m: 0.7723 - val_recall_m: 0.7360\n",
      "Epoch 26/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.3745 - acc: 0.9101 - f1_m: 0.9113 - precision_m: 0.9265 - recall_m: 0.8970 - val_loss: 0.7929 - val_acc: 0.7697 - val_f1_m: 0.7642 - val_precision_m: 0.7831 - val_recall_m: 0.7472\n",
      "Epoch 27/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.3429 - acc: 0.9195 - f1_m: 0.9260 - precision_m: 0.9471 - recall_m: 0.9064 - val_loss: 0.9642 - val_acc: 0.7528 - val_f1_m: 0.7477 - val_precision_m: 0.7541 - val_recall_m: 0.7416\n",
      "Epoch 28/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.4658 - acc: 0.8970 - f1_m: 0.8890 - precision_m: 0.9044 - recall_m: 0.8745 - val_loss: 0.9578 - val_acc: 0.7697 - val_f1_m: 0.7672 - val_precision_m: 0.7765 - val_recall_m: 0.7584\n",
      "Epoch 29/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.3597 - acc: 0.8933 - f1_m: 0.8949 - precision_m: 0.9106 - recall_m: 0.8801 - val_loss: 0.9392 - val_acc: 0.7528 - val_f1_m: 0.7586 - val_precision_m: 0.7707 - val_recall_m: 0.7472\n",
      "Epoch 30/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.3703 - acc: 0.9157 - f1_m: 0.9142 - precision_m: 0.9285 - recall_m: 0.9007 - val_loss: 0.8735 - val_acc: 0.7528 - val_f1_m: 0.7503 - val_precision_m: 0.7719 - val_recall_m: 0.7303\n",
      "Epoch 31/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.3060 - acc: 0.9082 - f1_m: 0.9085 - precision_m: 0.9229 - recall_m: 0.8951 - val_loss: 0.8584 - val_acc: 0.7472 - val_f1_m: 0.7505 - val_precision_m: 0.7665 - val_recall_m: 0.7360\n",
      "Epoch 32/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.3549 - acc: 0.9157 - f1_m: 0.9207 - precision_m: 0.9381 - recall_m: 0.9045 - val_loss: 0.9308 - val_acc: 0.7472 - val_f1_m: 0.7554 - val_precision_m: 0.7641 - val_recall_m: 0.7472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.3129 - acc: 0.9157 - f1_m: 0.9173 - precision_m: 0.9290 - recall_m: 0.9064 - val_loss: 0.9189 - val_acc: 0.7640 - val_f1_m: 0.7647 - val_precision_m: 0.7711 - val_recall_m: 0.7584\n",
      "Epoch 34/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2743 - acc: 0.9307 - f1_m: 0.9327 - precision_m: 0.9445 - recall_m: 0.9213 - val_loss: 0.9455 - val_acc: 0.7978 - val_f1_m: 0.7996 - val_precision_m: 0.8075 - val_recall_m: 0.7921\n",
      "Epoch 35/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.3271 - acc: 0.9326 - f1_m: 0.9322 - precision_m: 0.9459 - recall_m: 0.9195 - val_loss: 0.9927 - val_acc: 0.7416 - val_f1_m: 0.7507 - val_precision_m: 0.7604 - val_recall_m: 0.7416\n",
      "Epoch 36/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2419 - acc: 0.9457 - f1_m: 0.9419 - precision_m: 0.9537 - recall_m: 0.9307 - val_loss: 1.0158 - val_acc: 0.7584 - val_f1_m: 0.7541 - val_precision_m: 0.7672 - val_recall_m: 0.7416\n",
      "Epoch 37/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2499 - acc: 0.9251 - f1_m: 0.9287 - precision_m: 0.9403 - recall_m: 0.9176 - val_loss: 1.0143 - val_acc: 0.8034 - val_f1_m: 0.7863 - val_precision_m: 0.7982 - val_recall_m: 0.7753\n",
      "Epoch 38/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.3737 - acc: 0.9139 - f1_m: 0.9162 - precision_m: 0.9357 - recall_m: 0.8989 - val_loss: 1.1276 - val_acc: 0.6685 - val_f1_m: 0.6686 - val_precision_m: 0.7087 - val_recall_m: 0.6348\n",
      "Epoch 39/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.4218 - acc: 0.9007 - f1_m: 0.8976 - precision_m: 0.9161 - recall_m: 0.8801 - val_loss: 0.9347 - val_acc: 0.7753 - val_f1_m: 0.7797 - val_precision_m: 0.7964 - val_recall_m: 0.7640\n",
      "Epoch 40/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2413 - acc: 0.9251 - f1_m: 0.9251 - precision_m: 0.9449 - recall_m: 0.9064 - val_loss: 0.8842 - val_acc: 0.7640 - val_f1_m: 0.7635 - val_precision_m: 0.7746 - val_recall_m: 0.7528\n",
      "Epoch 41/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2307 - acc: 0.9326 - f1_m: 0.9331 - precision_m: 0.9395 - recall_m: 0.9270 - val_loss: 0.9642 - val_acc: 0.7640 - val_f1_m: 0.7739 - val_precision_m: 0.7902 - val_recall_m: 0.7584\n",
      "Epoch 42/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2272 - acc: 0.9513 - f1_m: 0.9525 - precision_m: 0.9618 - recall_m: 0.9438 - val_loss: 1.0313 - val_acc: 0.7360 - val_f1_m: 0.7461 - val_precision_m: 0.7570 - val_recall_m: 0.7360\n",
      "Epoch 43/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2084 - acc: 0.9288 - f1_m: 0.9283 - precision_m: 0.9416 - recall_m: 0.9157 - val_loss: 1.0262 - val_acc: 0.7697 - val_f1_m: 0.7756 - val_precision_m: 0.7819 - val_recall_m: 0.7697\n",
      "Epoch 44/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1685 - acc: 0.9607 - f1_m: 0.9572 - precision_m: 0.9653 - recall_m: 0.9494 - val_loss: 1.0681 - val_acc: 0.7360 - val_f1_m: 0.7389 - val_precision_m: 0.7477 - val_recall_m: 0.7303\n",
      "Epoch 45/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1472 - acc: 0.9588 - f1_m: 0.9622 - precision_m: 0.9696 - recall_m: 0.9551 - val_loss: 1.0359 - val_acc: 0.7640 - val_f1_m: 0.7692 - val_precision_m: 0.7807 - val_recall_m: 0.7584\n",
      "Epoch 46/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.1762 - acc: 0.9625 - f1_m: 0.9594 - precision_m: 0.9659 - recall_m: 0.9532 - val_loss: 1.0370 - val_acc: 0.7640 - val_f1_m: 0.7592 - val_precision_m: 0.7659 - val_recall_m: 0.7528\n",
      "Epoch 47/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1623 - acc: 0.9625 - f1_m: 0.9695 - precision_m: 0.9809 - recall_m: 0.9588 - val_loss: 1.0308 - val_acc: 0.7528 - val_f1_m: 0.7544 - val_precision_m: 0.7682 - val_recall_m: 0.7416\n",
      "Epoch 48/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1192 - acc: 0.9757 - f1_m: 0.9726 - precision_m: 0.9791 - recall_m: 0.9663 - val_loss: 1.0875 - val_acc: 0.7528 - val_f1_m: 0.7540 - val_precision_m: 0.7673 - val_recall_m: 0.7416\n",
      "Epoch 49/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1484 - acc: 0.9607 - f1_m: 0.9650 - precision_m: 0.9715 - recall_m: 0.9588 - val_loss: 1.0440 - val_acc: 0.7697 - val_f1_m: 0.7769 - val_precision_m: 0.7904 - val_recall_m: 0.7640\n",
      "Epoch 50/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.1217 - acc: 0.9775 - f1_m: 0.9783 - precision_m: 0.9811 - recall_m: 0.9757 - val_loss: 1.0787 - val_acc: 0.7697 - val_f1_m: 0.7709 - val_precision_m: 0.7780 - val_recall_m: 0.7640\n",
      "Epoch 51/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1427 - acc: 0.9682 - f1_m: 0.9690 - precision_m: 0.9737 - recall_m: 0.9644 - val_loss: 1.0774 - val_acc: 0.7528 - val_f1_m: 0.7515 - val_precision_m: 0.7559 - val_recall_m: 0.7472\n",
      "Epoch 52/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1709 - acc: 0.9569 - f1_m: 0.9565 - precision_m: 0.9620 - recall_m: 0.9513 - val_loss: 1.1417 - val_acc: 0.7865 - val_f1_m: 0.7876 - val_precision_m: 0.7945 - val_recall_m: 0.7809\n",
      "Epoch 53/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.1239 - acc: 0.9757 - f1_m: 0.9736 - precision_m: 0.9773 - recall_m: 0.9700 - val_loss: 1.1149 - val_acc: 0.7640 - val_f1_m: 0.7709 - val_precision_m: 0.7780 - val_recall_m: 0.7640\n",
      "Epoch 54/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.1130 - acc: 0.9644 - f1_m: 0.9660 - precision_m: 0.9715 - recall_m: 0.9607 - val_loss: 1.1395 - val_acc: 0.7809 - val_f1_m: 0.7765 - val_precision_m: 0.7835 - val_recall_m: 0.7697\n",
      "Epoch 55/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.1349 - acc: 0.9663 - f1_m: 0.9642 - precision_m: 0.9679 - recall_m: 0.9607 - val_loss: 1.1167 - val_acc: 0.7640 - val_f1_m: 0.7646 - val_precision_m: 0.7710 - val_recall_m: 0.7584\n",
      "Epoch 56/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1534 - acc: 0.9757 - f1_m: 0.9717 - precision_m: 0.9772 - recall_m: 0.9663 - val_loss: 1.1117 - val_acc: 0.7921 - val_f1_m: 0.7912 - val_precision_m: 0.7959 - val_recall_m: 0.7865\n",
      "Epoch 57/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1316 - acc: 0.9700 - f1_m: 0.9689 - precision_m: 0.9754 - recall_m: 0.9625 - val_loss: 1.2226 - val_acc: 0.7360 - val_f1_m: 0.7311 - val_precision_m: 0.7441 - val_recall_m: 0.7191\n",
      "Epoch 58/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1656 - acc: 0.9738 - f1_m: 0.9736 - precision_m: 0.9773 - recall_m: 0.9700 - val_loss: 1.0809 - val_acc: 0.7753 - val_f1_m: 0.7763 - val_precision_m: 0.7831 - val_recall_m: 0.7697\n",
      "Epoch 59/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1868 - acc: 0.9644 - f1_m: 0.9620 - precision_m: 0.9714 - recall_m: 0.9532 - val_loss: 1.2155 - val_acc: 0.7416 - val_f1_m: 0.7416 - val_precision_m: 0.7476 - val_recall_m: 0.7360\n",
      "Epoch 60/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2898 - acc: 0.9157 - f1_m: 0.9189 - precision_m: 0.9243 - recall_m: 0.9139 - val_loss: 1.1480 - val_acc: 0.7584 - val_f1_m: 0.7626 - val_precision_m: 0.7669 - val_recall_m: 0.7584\n",
      "Epoch 61/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2428 - acc: 0.9551 - f1_m: 0.9552 - precision_m: 0.9673 - recall_m: 0.9438 - val_loss: 1.0325 - val_acc: 0.7584 - val_f1_m: 0.7485 - val_precision_m: 0.7618 - val_recall_m: 0.7360\n",
      "Epoch 62/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1938 - acc: 0.9457 - f1_m: 0.9471 - precision_m: 0.9544 - recall_m: 0.9401 - val_loss: 1.0232 - val_acc: 0.7809 - val_f1_m: 0.7820 - val_precision_m: 0.7889 - val_recall_m: 0.7753\n",
      "Epoch 63/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1371 - acc: 0.9700 - f1_m: 0.9719 - precision_m: 0.9738 - recall_m: 0.9700 - val_loss: 1.0685 - val_acc: 0.7528 - val_f1_m: 0.7502 - val_precision_m: 0.7592 - val_recall_m: 0.7416\n",
      "Epoch 64/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1042 - acc: 0.9738 - f1_m: 0.9737 - precision_m: 0.9756 - recall_m: 0.9719 - val_loss: 1.1052 - val_acc: 0.7697 - val_f1_m: 0.7726 - val_precision_m: 0.7815 - val_recall_m: 0.7640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1017 - acc: 0.9794 - f1_m: 0.9793 - precision_m: 0.9831 - recall_m: 0.9757 - val_loss: 1.1248 - val_acc: 0.7640 - val_f1_m: 0.7627 - val_precision_m: 0.7671 - val_recall_m: 0.7584\n",
      "Epoch 66/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1377 - acc: 0.9700 - f1_m: 0.9709 - precision_m: 0.9737 - recall_m: 0.9682 - val_loss: 1.2480 - val_acc: 0.7303 - val_f1_m: 0.7383 - val_precision_m: 0.7467 - val_recall_m: 0.7303\n",
      "Epoch 67/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1055 - acc: 0.9757 - f1_m: 0.9756 - precision_m: 0.9775 - recall_m: 0.9738 - val_loss: 1.1454 - val_acc: 0.7640 - val_f1_m: 0.7652 - val_precision_m: 0.7722 - val_recall_m: 0.7584\n",
      "Epoch 68/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0881 - acc: 0.9738 - f1_m: 0.9727 - precision_m: 0.9754 - recall_m: 0.9700 - val_loss: 1.1344 - val_acc: 0.7472 - val_f1_m: 0.7516 - val_precision_m: 0.7561 - val_recall_m: 0.7472\n",
      "Epoch 69/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0693 - acc: 0.9831 - f1_m: 0.9859 - precision_m: 0.9906 - recall_m: 0.9813 - val_loss: 1.1879 - val_acc: 0.7697 - val_f1_m: 0.7764 - val_precision_m: 0.7835 - val_recall_m: 0.7697\n",
      "Epoch 70/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0640 - acc: 0.9850 - f1_m: 0.9868 - precision_m: 0.9886 - recall_m: 0.9850 - val_loss: 1.2401 - val_acc: 0.7697 - val_f1_m: 0.7721 - val_precision_m: 0.7746 - val_recall_m: 0.7697\n",
      "Epoch 71/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0584 - acc: 0.9831 - f1_m: 0.9868 - precision_m: 0.9906 - recall_m: 0.9831 - val_loss: 1.2497 - val_acc: 0.7697 - val_f1_m: 0.7689 - val_precision_m: 0.7742 - val_recall_m: 0.7640\n",
      "Epoch 72/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.2197 - acc: 0.9738 - f1_m: 0.9744 - precision_m: 0.9790 - recall_m: 0.9700 - val_loss: 1.5611 - val_acc: 0.6348 - val_f1_m: 0.6452 - val_precision_m: 0.6561 - val_recall_m: 0.6348\n",
      "Epoch 73/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1791 - acc: 0.9419 - f1_m: 0.9364 - precision_m: 0.9485 - recall_m: 0.9251 - val_loss: 1.1148 - val_acc: 0.7640 - val_f1_m: 0.7651 - val_precision_m: 0.7721 - val_recall_m: 0.7584\n",
      "Epoch 74/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.1153 - acc: 0.9738 - f1_m: 0.9735 - precision_m: 0.9791 - recall_m: 0.9682 - val_loss: 1.1080 - val_acc: 0.7303 - val_f1_m: 0.7224 - val_precision_m: 0.7319 - val_recall_m: 0.7135\n",
      "Epoch 75/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1513 - acc: 0.9551 - f1_m: 0.9546 - precision_m: 0.9619 - recall_m: 0.9476 - val_loss: 1.2320 - val_acc: 0.6742 - val_f1_m: 0.6743 - val_precision_m: 0.6804 - val_recall_m: 0.6685\n",
      "Epoch 76/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1319 - acc: 0.9663 - f1_m: 0.9643 - precision_m: 0.9661 - recall_m: 0.9625 - val_loss: 1.0468 - val_acc: 0.7865 - val_f1_m: 0.7938 - val_precision_m: 0.8015 - val_recall_m: 0.7865\n",
      "Epoch 77/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0870 - acc: 0.9813 - f1_m: 0.9840 - precision_m: 0.9869 - recall_m: 0.9813 - val_loss: 1.0495 - val_acc: 0.7809 - val_f1_m: 0.7858 - val_precision_m: 0.7910 - val_recall_m: 0.7809\n",
      "Epoch 78/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0672 - acc: 0.9775 - f1_m: 0.9765 - precision_m: 0.9793 - recall_m: 0.9738 - val_loss: 1.1034 - val_acc: 0.7640 - val_f1_m: 0.7689 - val_precision_m: 0.7742 - val_recall_m: 0.7640\n",
      "Epoch 79/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.1277 - acc: 0.9719 - f1_m: 0.9719 - precision_m: 0.9737 - recall_m: 0.9700 - val_loss: 1.1128 - val_acc: 0.7528 - val_f1_m: 0.7622 - val_precision_m: 0.7723 - val_recall_m: 0.7528\n",
      "Epoch 80/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0631 - acc: 0.9906 - f1_m: 0.9906 - precision_m: 0.9925 - recall_m: 0.9888 - val_loss: 1.1124 - val_acc: 0.7809 - val_f1_m: 0.7860 - val_precision_m: 0.7914 - val_recall_m: 0.7809\n",
      "Epoch 81/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0533 - acc: 0.9850 - f1_m: 0.9858 - precision_m: 0.9886 - recall_m: 0.9831 - val_loss: 1.1851 - val_acc: 0.7697 - val_f1_m: 0.7789 - val_precision_m: 0.7886 - val_recall_m: 0.7697\n",
      "Epoch 82/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0642 - acc: 0.9869 - f1_m: 0.9877 - precision_m: 0.9905 - recall_m: 0.9850 - val_loss: 1.1975 - val_acc: 0.7697 - val_f1_m: 0.7721 - val_precision_m: 0.7746 - val_recall_m: 0.7697\n",
      "Epoch 83/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.0615 - acc: 0.9757 - f1_m: 0.9793 - precision_m: 0.9830 - recall_m: 0.9757 - val_loss: 1.1866 - val_acc: 0.7753 - val_f1_m: 0.7753 - val_precision_m: 0.7753 - val_recall_m: 0.7753\n",
      "Epoch 84/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0498 - acc: 0.9906 - f1_m: 0.9897 - precision_m: 0.9925 - recall_m: 0.9869 - val_loss: 1.2227 - val_acc: 0.7640 - val_f1_m: 0.7665 - val_precision_m: 0.7689 - val_recall_m: 0.7640\n",
      "Epoch 85/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0635 - acc: 0.9813 - f1_m: 0.9812 - precision_m: 0.9831 - recall_m: 0.9794 - val_loss: 1.2495 - val_acc: 0.7809 - val_f1_m: 0.7737 - val_precision_m: 0.7780 - val_recall_m: 0.7697\n",
      "Epoch 86/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0850 - acc: 0.9757 - f1_m: 0.9746 - precision_m: 0.9754 - recall_m: 0.9738 - val_loss: 1.3264 - val_acc: 0.7135 - val_f1_m: 0.6967 - val_precision_m: 0.7088 - val_recall_m: 0.6854\n",
      "Epoch 87/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.0800 - acc: 0.9775 - f1_m: 0.9775 - precision_m: 0.9793 - recall_m: 0.9757 - val_loss: 1.2551 - val_acc: 0.7697 - val_f1_m: 0.7685 - val_precision_m: 0.7731 - val_recall_m: 0.7640\n",
      "Epoch 88/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0351 - acc: 0.9944 - f1_m: 0.9934 - precision_m: 0.9963 - recall_m: 0.9906 - val_loss: 1.2365 - val_acc: 0.7865 - val_f1_m: 0.7853 - val_precision_m: 0.7898 - val_recall_m: 0.7809\n",
      "Epoch 89/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0488 - acc: 0.9906 - f1_m: 0.9916 - precision_m: 0.9925 - recall_m: 0.9906 - val_loss: 1.2616 - val_acc: 0.7753 - val_f1_m: 0.7741 - val_precision_m: 0.7786 - val_recall_m: 0.7697\n",
      "Epoch 90/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0424 - acc: 0.9925 - f1_m: 0.9934 - precision_m: 0.9963 - recall_m: 0.9906 - val_loss: 1.2313 - val_acc: 0.7921 - val_f1_m: 0.7887 - val_precision_m: 0.7909 - val_recall_m: 0.7865\n",
      "Epoch 91/100\n",
      "534/534 [==============================] - 2s 5ms/step - loss: 0.0396 - acc: 0.9888 - f1_m: 0.9888 - precision_m: 0.9888 - recall_m: 0.9888 - val_loss: 1.2201 - val_acc: 0.7809 - val_f1_m: 0.7774 - val_precision_m: 0.7796 - val_recall_m: 0.7753\n",
      "Epoch 92/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0316 - acc: 0.9925 - f1_m: 0.9934 - precision_m: 0.9944 - recall_m: 0.9925 - val_loss: 1.2514 - val_acc: 0.7809 - val_f1_m: 0.7774 - val_precision_m: 0.7796 - val_recall_m: 0.7753\n",
      "Epoch 93/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0315 - acc: 0.9944 - f1_m: 0.9915 - precision_m: 0.9944 - recall_m: 0.9888 - val_loss: 1.2767 - val_acc: 0.7865 - val_f1_m: 0.7895 - val_precision_m: 0.7984 - val_recall_m: 0.7809\n",
      "Epoch 94/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0686 - acc: 0.9869 - f1_m: 0.9859 - precision_m: 0.9869 - recall_m: 0.9850 - val_loss: 1.3038 - val_acc: 0.7528 - val_f1_m: 0.7491 - val_precision_m: 0.7510 - val_recall_m: 0.7472\n",
      "Epoch 95/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0630 - acc: 0.9869 - f1_m: 0.9869 - precision_m: 0.9888 - recall_m: 0.9850 - val_loss: 1.3626 - val_acc: 0.7472 - val_f1_m: 0.7472 - val_precision_m: 0.7472 - val_recall_m: 0.7472\n",
      "Epoch 96/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0454 - acc: 0.9906 - f1_m: 0.9897 - precision_m: 0.9906 - recall_m: 0.9888 - val_loss: 1.3108 - val_acc: 0.7809 - val_f1_m: 0.7854 - val_precision_m: 0.7900 - val_recall_m: 0.7809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0293 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 1.3389 - val_acc: 0.7697 - val_f1_m: 0.7717 - val_precision_m: 0.7738 - val_recall_m: 0.7697\n",
      "Epoch 98/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0229 - acc: 0.9963 - f1_m: 0.9972 - precision_m: 0.9981 - recall_m: 0.9963 - val_loss: 1.3507 - val_acc: 0.7753 - val_f1_m: 0.7717 - val_precision_m: 0.7738 - val_recall_m: 0.7697\n",
      "Epoch 99/100\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 0.0307 - acc: 0.9963 - f1_m: 0.9971 - precision_m: 0.9981 - recall_m: 0.9963 - val_loss: 1.3456 - val_acc: 0.7753 - val_f1_m: 0.7753 - val_precision_m: 0.7753 - val_recall_m: 0.7753\n",
      "Epoch 100/100\n",
      "534/534 [==============================] - 2s 4ms/step - loss: 0.0294 - acc: 0.9925 - f1_m: 0.9934 - precision_m: 0.9943 - recall_m: 0.9925 - val_loss: 1.3560 - val_acc: 0.7753 - val_f1_m: 0.7774 - val_precision_m: 0.7796 - val_recall_m: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f4b08fa7f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.5))\n",
    "# model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "# model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(Bidirectional(LSTM(100)))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Dense(4, activation='softmax'))\n",
    "# model_glove.add(Dense(4, activation='sigmoid'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "## Fit train data\n",
    "\n",
    "# set class_weight for imbalanced classes\n",
    "class_weight={0:1,1:1,2:10,3:10}\n",
    "\n",
    "\n",
    "model_glove.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs = 100,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.92      0.85        98\n",
      "           1       0.90      0.68      0.77        53\n",
      "           2       0.62      0.65      0.64        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.79      0.79      0.79       178\n",
      "   macro avg       0.58      0.56      0.57       178\n",
      "weighted avg       0.78      0.79      0.78       178\n",
      "\n",
      "[[90  3  5  0]\n",
      " [14 36  3  0]\n",
      " [ 7  1 15  0]\n",
      " [ 3  0  1  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "## from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_prob = model_glove.predict(X_test)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(stroke_train, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "y_train_cat= np_utils.to_categorical(y_train)\n",
    "y_test_cat= np_utils.to_categorical(y_test)\n",
    "y_val_cat= np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 100), (178, 100), (534, 4), (178, 4))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train_cat.shape,y_test_cat.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/30\n",
      "534/534 [==============================] - 4s 7ms/step - loss: 1.7363 - acc: 0.3446 - f1_m: 0.0694 - precision_m: 0.4168 - recall_m: 0.0393 - val_loss: 1.2083 - val_acc: 0.5730 - val_f1_m: 0.3379 - val_precision_m: 0.5528 - val_recall_m: 0.2472\n",
      "Epoch 2/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 1.4361 - acc: 0.5712 - f1_m: 0.4008 - precision_m: 0.6729 - recall_m: 0.2903 - val_loss: 1.1201 - val_acc: 0.6236 - val_f1_m: 0.3388 - val_precision_m: 0.6301 - val_recall_m: 0.2360\n",
      "Epoch 3/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 1.2514 - acc: 0.6423 - f1_m: 0.4493 - precision_m: 0.7698 - recall_m: 0.3240 - val_loss: 0.9977 - val_acc: 0.6685 - val_f1_m: 0.5583 - val_precision_m: 0.7461 - val_recall_m: 0.4494\n",
      "Epoch 4/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 1.1339 - acc: 0.6985 - f1_m: 0.6106 - precision_m: 0.7862 - recall_m: 0.5019 - val_loss: 0.9156 - val_acc: 0.6910 - val_f1_m: 0.6299 - val_precision_m: 0.7841 - val_recall_m: 0.5281\n",
      "Epoch 5/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.9333 - acc: 0.7434 - f1_m: 0.6951 - precision_m: 0.8392 - recall_m: 0.5955 - val_loss: 0.8142 - val_acc: 0.7191 - val_f1_m: 0.7086 - val_precision_m: 0.7865 - val_recall_m: 0.6461\n",
      "Epoch 6/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.7781 - acc: 0.7884 - f1_m: 0.7666 - precision_m: 0.8523 - recall_m: 0.6985 - val_loss: 0.8048 - val_acc: 0.7022 - val_f1_m: 0.7007 - val_precision_m: 0.7865 - val_recall_m: 0.6348\n",
      "Epoch 7/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.6690 - acc: 0.8015 - f1_m: 0.7773 - precision_m: 0.8579 - recall_m: 0.7135 - val_loss: 0.8380 - val_acc: 0.6573 - val_f1_m: 0.6687 - val_precision_m: 0.7690 - val_recall_m: 0.5955\n",
      "Epoch 8/30\n",
      "534/534 [==============================] - ETA: 0s - loss: 0.6065 - acc: 0.8320 - f1_m: 0.8214 - precision_m: 0.8802 - recall_m: 0.773 - 1s 2ms/step - loss: 0.5990 - acc: 0.8333 - f1_m: 0.8223 - precision_m: 0.8852 - recall_m: 0.7715 - val_loss: 0.8271 - val_acc: 0.6966 - val_f1_m: 0.6697 - val_precision_m: 0.7705 - val_recall_m: 0.5955\n",
      "Epoch 9/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.5103 - acc: 0.8427 - f1_m: 0.8378 - precision_m: 0.9018 - recall_m: 0.7846 - val_loss: 0.8403 - val_acc: 0.7022 - val_f1_m: 0.7213 - val_precision_m: 0.7879 - val_recall_m: 0.6685\n",
      "Epoch 10/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.4747 - acc: 0.8689 - f1_m: 0.8651 - precision_m: 0.9143 - recall_m: 0.8221 - val_loss: 0.8079 - val_acc: 0.7191 - val_f1_m: 0.7384 - val_precision_m: 0.7873 - val_recall_m: 0.6966\n",
      "Epoch 11/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.3713 - acc: 0.8745 - f1_m: 0.8690 - precision_m: 0.9174 - recall_m: 0.8277 - val_loss: 0.7801 - val_acc: 0.7303 - val_f1_m: 0.7211 - val_precision_m: 0.7624 - val_recall_m: 0.6854\n",
      "Epoch 12/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.3447 - acc: 0.8858 - f1_m: 0.8846 - precision_m: 0.9142 - recall_m: 0.8577 - val_loss: 0.7842 - val_acc: 0.7247 - val_f1_m: 0.7269 - val_precision_m: 0.7672 - val_recall_m: 0.6910\n",
      "Epoch 13/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.2703 - acc: 0.9232 - f1_m: 0.9221 - precision_m: 0.9428 - recall_m: 0.9026 - val_loss: 0.8976 - val_acc: 0.7079 - val_f1_m: 0.7181 - val_precision_m: 0.7422 - val_recall_m: 0.6966\n",
      "Epoch 14/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.2713 - acc: 0.9195 - f1_m: 0.9137 - precision_m: 0.9362 - recall_m: 0.8933 - val_loss: 0.8424 - val_acc: 0.7303 - val_f1_m: 0.7328 - val_precision_m: 0.7664 - val_recall_m: 0.7022\n",
      "Epoch 15/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.2164 - acc: 0.9419 - f1_m: 0.9392 - precision_m: 0.9623 - recall_m: 0.9176 - val_loss: 0.9078 - val_acc: 0.7247 - val_f1_m: 0.7220 - val_precision_m: 0.7508 - val_recall_m: 0.6966\n",
      "Epoch 16/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1800 - acc: 0.9419 - f1_m: 0.9458 - precision_m: 0.9577 - recall_m: 0.9345 - val_loss: 0.9072 - val_acc: 0.7640 - val_f1_m: 0.7438 - val_precision_m: 0.7713 - val_recall_m: 0.7191\n",
      "Epoch 17/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1568 - acc: 0.9551 - f1_m: 0.9600 - precision_m: 0.9710 - recall_m: 0.9494 - val_loss: 0.8507 - val_acc: 0.7640 - val_f1_m: 0.7537 - val_precision_m: 0.7790 - val_recall_m: 0.7303\n",
      "Epoch 18/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1332 - acc: 0.9719 - f1_m: 0.9637 - precision_m: 0.9788 - recall_m: 0.9494 - val_loss: 0.8266 - val_acc: 0.7640 - val_f1_m: 0.7698 - val_precision_m: 0.7952 - val_recall_m: 0.7472\n",
      "Epoch 19/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1130 - acc: 0.9757 - f1_m: 0.9745 - precision_m: 0.9830 - recall_m: 0.9663 - val_loss: 0.9491 - val_acc: 0.7809 - val_f1_m: 0.7853 - val_precision_m: 0.7898 - val_recall_m: 0.7809\n",
      "Epoch 20/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0925 - acc: 0.9738 - f1_m: 0.9736 - precision_m: 0.9792 - recall_m: 0.9682 - val_loss: 0.9962 - val_acc: 0.7640 - val_f1_m: 0.7671 - val_precision_m: 0.7765 - val_recall_m: 0.7584\n",
      "Epoch 21/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1066 - acc: 0.9625 - f1_m: 0.9659 - precision_m: 0.9732 - recall_m: 0.9588 - val_loss: 0.9276 - val_acc: 0.7753 - val_f1_m: 0.7800 - val_precision_m: 0.7850 - val_recall_m: 0.7753\n",
      "Epoch 22/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1351 - acc: 0.9569 - f1_m: 0.9584 - precision_m: 0.9658 - recall_m: 0.9513 - val_loss: 0.9317 - val_acc: 0.7584 - val_f1_m: 0.7433 - val_precision_m: 0.7636 - val_recall_m: 0.7247\n",
      "Epoch 23/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.1290 - acc: 0.9551 - f1_m: 0.9548 - precision_m: 0.9603 - recall_m: 0.9494 - val_loss: 0.8694 - val_acc: 0.7753 - val_f1_m: 0.7745 - val_precision_m: 0.8051 - val_recall_m: 0.7472\n",
      "Epoch 24/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0925 - acc: 0.9775 - f1_m: 0.9784 - precision_m: 0.9812 - recall_m: 0.9757 - val_loss: 0.8950 - val_acc: 0.7865 - val_f1_m: 0.7974 - val_precision_m: 0.8214 - val_recall_m: 0.7753\n",
      "Epoch 25/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0557 - acc: 0.9888 - f1_m: 0.9859 - precision_m: 0.9906 - recall_m: 0.9813 - val_loss: 0.9183 - val_acc: 0.7978 - val_f1_m: 0.8007 - val_precision_m: 0.8096 - val_recall_m: 0.7921\n",
      "Epoch 26/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0560 - acc: 0.9869 - f1_m: 0.9887 - precision_m: 0.9906 - recall_m: 0.9869 - val_loss: 0.9684 - val_acc: 0.7753 - val_f1_m: 0.7739 - val_precision_m: 0.7782 - val_recall_m: 0.7697\n",
      "Epoch 27/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0541 - acc: 0.9906 - f1_m: 0.9887 - precision_m: 0.9906 - recall_m: 0.9869 - val_loss: 1.0746 - val_acc: 0.7528 - val_f1_m: 0.7597 - val_precision_m: 0.7730 - val_recall_m: 0.7472\n",
      "Epoch 28/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0577 - acc: 0.9813 - f1_m: 0.9812 - precision_m: 0.9849 - recall_m: 0.9775 - val_loss: 0.9803 - val_acc: 0.8034 - val_f1_m: 0.8042 - val_precision_m: 0.8111 - val_recall_m: 0.7978\n",
      "Epoch 29/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0433 - acc: 0.9944 - f1_m: 0.9934 - precision_m: 0.9944 - recall_m: 0.9925 - val_loss: 1.0091 - val_acc: 0.8034 - val_f1_m: 0.8000 - val_precision_m: 0.8023 - val_recall_m: 0.7978\n",
      "Epoch 30/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.9925 - f1_m: 0.9906 - precision_m: 0.9925 - recall_m: 0.9888 - val_loss: 0.9546 - val_acc: 0.7809 - val_f1_m: 0.7815 - val_precision_m: 0.7880 - val_recall_m: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1624e169ac8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "# model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "# model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(GRU(100))\n",
    "model_glove.add(Dropout(0.4))\n",
    "model_glove.add(Dense(4, activation='softmax'))\n",
    "# model_glove.add(Dense(4, activation='sigmoid'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "## Fit train data\n",
    "\n",
    "# set class_weight for imbalanced classes\n",
    "class_weight={0:1,1:1,2:1,3:10}\n",
    "\n",
    "\n",
    "model_glove.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs = 30,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82        98\n",
      "           1       0.75      0.72      0.73        53\n",
      "           2       0.70      0.70      0.70        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       178\n",
      "   macro avg       0.56      0.56      0.56       178\n",
      "weighted avg       0.75      0.76      0.76       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prob = model_glove.predict(X_test)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(stroke_train, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "y_train_cat= np_utils.to_categorical(y_train)\n",
    "y_test_cat= np_utils.to_categorical(y_test)\n",
    "y_val_cat= np_utils.to_categorical(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((534, 100), (178, 100), (534, 4), (178, 4))"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train_cat.shape,y_test_cat.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/15\n",
      " 32/534 [>.............................] - ETA: 57s - loss: 3.5664 - acc: 0.0938 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-260-23a113eee6be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel_glove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_cat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model_glove = Sequential()\n",
    "model_glove.add(Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "model_glove.add(Dropout(0.2))\n",
    "# model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "# model_glove.add(MaxPooling1D(pool_size=4))\n",
    "model_glove.add(LSTM(100))\n",
    "model_glove.add(Dropout(0.2))\n",
    "model_glove.add(Dense(4, activation='softmax'))\n",
    "# model_glove.add(Dense(4, activation='sigmoid'))\n",
    "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "## Fit train data\n",
    "\n",
    "# set class_weight for imbalanced classes\n",
    "class_weight={0:1,1:1,2:10,3:20}\n",
    "\n",
    "\n",
    "model_glove.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs = 15,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.73      0.77        98\n",
      "           1       0.89      0.45      0.60        53\n",
      "           2       0.31      0.83      0.45        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.65      0.65      0.65       178\n",
      "   macro avg       0.50      0.50      0.45       178\n",
      "weighted avg       0.75      0.65      0.66       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prob = model_glove.predict(X_test)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "EMBEDDING_DIM=300\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "\n",
    "# 模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接\n",
    "main_input = Input(shape=(100,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = Embedding(len(word_index) + 1, 300, input_length = 100, weights = [embedding_matrix], trainable = False)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = Convolution1D(128, 3, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn1 = MaxPool1D(pool_size=5)(cnn1)\n",
    "cnn2 = Convolution1D(128, 4, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn2 = MaxPool1D(pool_size=5)(cnn2)\n",
    "cnn3 = Convolution1D(128, 5, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn3 = MaxPool1D(pool_size=5)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "# cnn = concatenate([cnn1,cnn2], axis=-1)\n",
    "cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "flat = Flatten()(cnn)\n",
    "drop = Dropout(0.2)(flat)\n",
    "main_output = Dense(4, activation='softmax')(drop)\n",
    "model_cnn = Model(inputs = main_input, outputs = main_output)\n",
    "\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/30\n",
      "534/534 [==============================] - 3s 5ms/step - loss: 1.6495 - acc: 0.5787 - f1_m: 0.4043 - precision_m: 0.5526 - recall_m: 0.3521 - val_loss: 1.1841 - val_acc: 0.4663 - val_f1_m: 0.2398 - val_precision_m: 0.7551 - val_recall_m: 0.1461\n",
      "Epoch 2/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.9370 - acc: 0.7022 - f1_m: 0.6271 - precision_m: 0.8577 - recall_m: 0.5206 - val_loss: 0.8430 - val_acc: 0.7079 - val_f1_m: 0.6925 - val_precision_m: 0.7483 - val_recall_m: 0.6461\n",
      "Epoch 3/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.6009 - acc: 0.8502 - f1_m: 0.8431 - precision_m: 0.9072 - recall_m: 0.7884 - val_loss: 0.8031 - val_acc: 0.7247 - val_f1_m: 0.7210 - val_precision_m: 0.7770 - val_recall_m: 0.6742\n",
      "Epoch 4/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.3992 - acc: 0.9045 - f1_m: 0.9093 - precision_m: 0.9568 - recall_m: 0.8670 - val_loss: 0.7505 - val_acc: 0.7528 - val_f1_m: 0.7491 - val_precision_m: 0.7842 - val_recall_m: 0.7191\n",
      "Epoch 5/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.2570 - acc: 0.9532 - f1_m: 0.9479 - precision_m: 0.9726 - recall_m: 0.9251 - val_loss: 0.7329 - val_acc: 0.7640 - val_f1_m: 0.7547 - val_precision_m: 0.7892 - val_recall_m: 0.7247\n",
      "Epoch 6/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.1725 - acc: 0.9775 - f1_m: 0.9742 - precision_m: 0.9865 - recall_m: 0.9625 - val_loss: 0.7342 - val_acc: 0.7697 - val_f1_m: 0.7655 - val_precision_m: 0.7928 - val_recall_m: 0.7416\n",
      "Epoch 7/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.1138 - acc: 0.9944 - f1_m: 0.9895 - precision_m: 0.9963 - recall_m: 0.9831 - val_loss: 0.7386 - val_acc: 0.7640 - val_f1_m: 0.7664 - val_precision_m: 0.7875 - val_recall_m: 0.7472\n",
      "Epoch 8/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0817 - acc: 0.9963 - f1_m: 0.9915 - precision_m: 0.9963 - recall_m: 0.9869 - val_loss: 0.7694 - val_acc: 0.7640 - val_f1_m: 0.7672 - val_precision_m: 0.7896 - val_recall_m: 0.7472\n",
      "Epoch 9/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0566 - acc: 0.9981 - f1_m: 0.9962 - precision_m: 0.9981 - recall_m: 0.9944 - val_loss: 0.7677 - val_acc: 0.7640 - val_f1_m: 0.7680 - val_precision_m: 0.7844 - val_recall_m: 0.7528\n",
      "Epoch 10/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0462 - acc: 0.9981 - f1_m: 0.9981 - precision_m: 1.0000 - recall_m: 0.9963 - val_loss: 0.8001 - val_acc: 0.7584 - val_f1_m: 0.7649 - val_precision_m: 0.7845 - val_recall_m: 0.7472\n",
      "Epoch 11/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0374 - acc: 0.9981 - f1_m: 0.9962 - precision_m: 0.9981 - recall_m: 0.9944 - val_loss: 0.7948 - val_acc: 0.7753 - val_f1_m: 0.7718 - val_precision_m: 0.7864 - val_recall_m: 0.7584\n",
      "Epoch 12/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0322 - acc: 0.9981 - f1_m: 0.9972 - precision_m: 0.9981 - recall_m: 0.9963 - val_loss: 0.8346 - val_acc: 0.7697 - val_f1_m: 0.7744 - val_precision_m: 0.7915 - val_recall_m: 0.7584\n",
      "Epoch 13/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0240 - acc: 1.0000 - f1_m: 0.9981 - precision_m: 1.0000 - recall_m: 0.9963 - val_loss: 0.8282 - val_acc: 0.7753 - val_f1_m: 0.7708 - val_precision_m: 0.7905 - val_recall_m: 0.7528\n",
      "Epoch 14/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0208 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8426 - val_acc: 0.7584 - val_f1_m: 0.7641 - val_precision_m: 0.7825 - val_recall_m: 0.7472\n",
      "Epoch 15/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0173 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8679 - val_acc: 0.7753 - val_f1_m: 0.7721 - val_precision_m: 0.7870 - val_recall_m: 0.7584\n",
      "Epoch 16/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0139 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8645 - val_acc: 0.7753 - val_f1_m: 0.7719 - val_precision_m: 0.7866 - val_recall_m: 0.7584\n",
      "Epoch 17/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0141 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8788 - val_acc: 0.7753 - val_f1_m: 0.7738 - val_precision_m: 0.7904 - val_recall_m: 0.7584\n",
      "Epoch 18/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0109 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8913 - val_acc: 0.7753 - val_f1_m: 0.7776 - val_precision_m: 0.7922 - val_recall_m: 0.7640\n",
      "Epoch 19/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0112 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.8991 - val_acc: 0.7753 - val_f1_m: 0.7738 - val_precision_m: 0.7904 - val_recall_m: 0.7584\n",
      "Epoch 20/30\n",
      "534/534 [==============================] - 1s 1ms/step - loss: 0.0091 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9148 - val_acc: 0.7809 - val_f1_m: 0.7738 - val_precision_m: 0.7904 - val_recall_m: 0.7584\n",
      "Epoch 21/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0085 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9134 - val_acc: 0.7753 - val_f1_m: 0.7809 - val_precision_m: 0.7927 - val_recall_m: 0.7697\n",
      "Epoch 22/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0079 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9371 - val_acc: 0.7753 - val_f1_m: 0.7785 - val_precision_m: 0.7880 - val_recall_m: 0.7697\n",
      "Epoch 23/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0068 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9260 - val_acc: 0.7809 - val_f1_m: 0.7809 - val_precision_m: 0.7927 - val_recall_m: 0.7697\n",
      "Epoch 24/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0064 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9369 - val_acc: 0.7809 - val_f1_m: 0.7846 - val_precision_m: 0.7945 - val_recall_m: 0.7753\n",
      "Epoch 25/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9543 - val_acc: 0.7809 - val_f1_m: 0.7810 - val_precision_m: 0.7930 - val_recall_m: 0.7697\n",
      "Epoch 26/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9473 - val_acc: 0.7809 - val_f1_m: 0.7812 - val_precision_m: 0.7938 - val_recall_m: 0.7697\n",
      "Epoch 27/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9530 - val_acc: 0.7809 - val_f1_m: 0.7812 - val_precision_m: 0.7938 - val_recall_m: 0.7697\n",
      "Epoch 28/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9669 - val_acc: 0.7809 - val_f1_m: 0.7812 - val_precision_m: 0.7938 - val_recall_m: 0.7697\n",
      "Epoch 29/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0043 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9574 - val_acc: 0.7753 - val_f1_m: 0.7841 - val_precision_m: 0.7933 - val_recall_m: 0.7753\n",
      "Epoch 30/30\n",
      "534/534 [==============================] - 1s 2ms/step - loss: 0.0042 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 0.9779 - val_acc: 0.7809 - val_f1_m: 0.7810 - val_precision_m: 0.7930 - val_recall_m: 0.7697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16226cc4748>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight={0:1,1:1,2:1,3:10}\n",
    "\n",
    "model_cnn.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), epochs = 30,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81        98\n",
      "           1       0.82      0.62      0.71        53\n",
      "           2       0.86      0.26      0.40        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       178\n",
      "   macro avg       0.60      0.46      0.48       178\n",
      "weighted avg       0.75      0.74      0.71       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# old result\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prob = model_cnn.predict(X_test)\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "\n",
    "# confusion_matrix(y_test[int(len(X_test)/2):], y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totally 91 chars.\n",
      "char vector shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "char_map_vec={}\n",
    "\n",
    "with open('../data/char-embeddings.txt','r') as f:\n",
    "    for line in f:\n",
    "        char_map_vec[line[0]]=np.fromstring(line[1:],sep=' ')\n",
    "print('totally',len(char_map_vec),'chars.')\n",
    "print(\"char vector shape:\",char_map_vec['a'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max number of chars in twitter: 402\n"
     ]
    }
   ],
   "source": [
    "max_l=0\n",
    "l=0\n",
    "\n",
    "for s in df['tweet'].tolist():\n",
    "    if(len([c for c in s if c in char_map_vec])>max_l):\n",
    "        max_l=len([c for c in s if c in char_map_vec])\n",
    "    \n",
    "print('max number of chars in twitter:',max_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df_map_char_emb, key is the row index of original df file, value is the char embedding of each tweet\n",
    "\n",
    "df_map_char_emb={}\n",
    "for i,s in enumerate(df['tweet'].tolist()):\n",
    "    each_tweet_char_emb=np.zeros((402,300))\n",
    "    for j,c in enumerate([c for c in s if c in char_map_vec]):\n",
    "        each_tweet_char_emb[j]=char_map_vec[c]\n",
    "    df_map_char_emb[i]=each_tweet_char_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke_train=data[df_to_evaluate.index]\n",
    "stroke_label=df_to_evaluate.label.tolist()\n",
    "\n",
    "char_train=[df_map_char_emb[i] for i in df_to_evaluate.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(890, 100)\n",
      "(890, 402, 300)\n"
     ]
    }
   ],
   "source": [
    "print(stroke_train.shape)\n",
    "char_train=np.array(char_train)\n",
    "print(char_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(534, 402, 300) (178, 402, 300) (178, 402, 300) (534, 100)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(stroke_train, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "X_val,X_test,y_val,y_test = train_test_split(X_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "# print(y_test)\n",
    "y_train_cat= np_utils.to_categorical(y_train)\n",
    "y_test_cat= np_utils.to_categorical(y_test)\n",
    "y_val_cat= np_utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "X_char_train, X_char_val_test, y_train, y_val_test = train_test_split(char_train, stroke_label,\n",
    "                                                    stratify=stroke_label, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "X_char_val,X_char_test,y_val,y_test = train_test_split(X_char_val_test, y_val_test,\n",
    "                                                    stratify=y_val_test, \n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=0\n",
    "                                                   )\n",
    "\n",
    "print(X_char_train.shape,X_char_val.shape,X_char_test.shape,X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn-char model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer to [paper](https://www.aclweb.org/anthology/W19-5025.pdf)  \n",
    "l1 = char + convo layers  \n",
    "l2 = word emb + convo layers  \n",
    "concat l1, l2 to form input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "EMBEDDING_DIM=300\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Activation, merge, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "# part 1 of model: the cnn of the word embeddings\n",
    "# 模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接\n",
    "main_input = Input(shape=(100,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = Embedding(len(word_index) + 1, 300, input_length = 100, weights = [embedding_matrix], trainable = False)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = Convolution1D(128, 3, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn1 = MaxPool1D(pool_size=5)(cnn1)\n",
    "cnn2 = Convolution1D(128, 4, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn2 = MaxPool1D(pool_size=5)(cnn2)\n",
    "cnn3 = Convolution1D(128, 5, padding='same', strides = 2, activation='relu')(embed)\n",
    "cnn3 = MaxPool1D(pool_size=5)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "\n",
    "# part 2 of model: the cnn of the char embeddings\n",
    "char_input=Input(shape=(402,300),dtype='float32')\n",
    "char_cnn1 = Convolution1D(128, 3, padding='same', strides = 2, activation='relu')(char_input)\n",
    "char_cnn1 = MaxPool1D(pool_size=5)(char_cnn1)\n",
    "char_cnn2 = Convolution1D(128, 4, padding='same', strides = 2, activation='relu')(char_input)\n",
    "char_cnn2 = MaxPool1D(pool_size=5)(char_cnn2)\n",
    "# char_cnn3 = Convolution1D(128, 5, padding='same', strides = 2, activation='relu')(char_input)\n",
    "# char_cnn3 = MaxPool1D(pool_size=5)(char_cnn3)\n",
    "\n",
    "\n",
    "# part 3: integration of the model\n",
    "cnn = concatenate([cnn1,cnn2,cnn3], axis=-1)\n",
    "# char_cnn = concatenate([char_cnn1,char_cnn2,char_cnn3], axis=-1)\n",
    "char_cnn = concatenate([char_cnn1,char_cnn2], axis=-1)\n",
    "# char_cnn = char_cnn1\n",
    "flat_1 = Flatten()(cnn)\n",
    "flat_2 = Flatten()(char_cnn)\n",
    "flat = concatenate([flat_1,flat_2], axis=-1)\n",
    "drop = Dropout(0.2)(flat)\n",
    "\n",
    "\n",
    "main_output = Dense(4, activation='softmax')(drop)\n",
    "model_cnn = Model(inputs = [main_input,char_input], outputs = main_output)\n",
    "\n",
    "model_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "# model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/15\n",
      "534/534 [==============================] - 7s 14ms/step - loss: 3.4294 - acc: 0.1816 - f1_m: 0.1186 - precision_m: 0.1920 - recall_m: 0.0880 - val_loss: 1.5979 - val_acc: 0.1348 - val_f1_m: 0.1203 - val_precision_m: 0.1630 - val_recall_m: 0.0955\n",
      "Epoch 2/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 2.0047 - acc: 0.4813 - f1_m: 0.3556 - precision_m: 0.6058 - recall_m: 0.2678 - val_loss: 1.6449 - val_acc: 0.1461 - val_f1_m: 0.1575 - val_precision_m: 0.2047 - val_recall_m: 0.1292\n",
      "Epoch 3/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 1.3122 - acc: 0.6929 - f1_m: 0.6100 - precision_m: 0.8464 - recall_m: 0.4869 - val_loss: 1.1147 - val_acc: 0.5112 - val_f1_m: 0.4014 - val_precision_m: 0.5954 - val_recall_m: 0.3034\n",
      "Epoch 4/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.7756 - acc: 0.8783 - f1_m: 0.8511 - precision_m: 0.9538 - recall_m: 0.7715 - val_loss: 0.9852 - val_acc: 0.6573 - val_f1_m: 0.5802 - val_precision_m: 0.7695 - val_recall_m: 0.4719\n",
      "Epoch 5/15\n",
      "534/534 [==============================] - 4s 7ms/step - loss: 0.4664 - acc: 0.9157 - f1_m: 0.9137 - precision_m: 0.9501 - recall_m: 0.8820 - val_loss: 0.8381 - val_acc: 0.7472 - val_f1_m: 0.7194 - val_precision_m: 0.7871 - val_recall_m: 0.6629\n",
      "Epoch 6/15\n",
      "534/534 [==============================] - 4s 7ms/step - loss: 0.2947 - acc: 0.9625 - f1_m: 0.9610 - precision_m: 0.9750 - recall_m: 0.9476 - val_loss: 0.8296 - val_acc: 0.7472 - val_f1_m: 0.7367 - val_precision_m: 0.8005 - val_recall_m: 0.6854\n",
      "Epoch 7/15\n",
      "534/534 [==============================] - 4s 8ms/step - loss: 0.1972 - acc: 0.9719 - f1_m: 0.9753 - precision_m: 0.9848 - recall_m: 0.9663 - val_loss: 0.8226 - val_acc: 0.7584 - val_f1_m: 0.7497 - val_precision_m: 0.7772 - val_recall_m: 0.7247\n",
      "Epoch 8/15\n",
      "534/534 [==============================] - 5s 9ms/step - loss: 0.1320 - acc: 0.9850 - f1_m: 0.9838 - precision_m: 0.9923 - recall_m: 0.9757 - val_loss: 0.8246 - val_acc: 0.7640 - val_f1_m: 0.7625 - val_precision_m: 0.7852 - val_recall_m: 0.7416\n",
      "Epoch 9/15\n",
      "534/534 [==============================] - 4s 7ms/step - loss: 0.1012 - acc: 0.9906 - f1_m: 0.9924 - precision_m: 0.9980 - recall_m: 0.9869 - val_loss: 0.8305 - val_acc: 0.7697 - val_f1_m: 0.7530 - val_precision_m: 0.7785 - val_recall_m: 0.7303\n",
      "Epoch 10/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.0775 - acc: 0.9944 - f1_m: 0.9933 - precision_m: 0.9981 - recall_m: 0.9888 - val_loss: 0.8432 - val_acc: 0.7753 - val_f1_m: 0.7631 - val_precision_m: 0.7870 - val_recall_m: 0.7416\n",
      "Epoch 11/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.0577 - acc: 0.9981 - f1_m: 0.9971 - precision_m: 1.0000 - recall_m: 0.9944 - val_loss: 0.8436 - val_acc: 0.7640 - val_f1_m: 0.7534 - val_precision_m: 0.7788 - val_recall_m: 0.7303\n",
      "Epoch 12/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.0479 - acc: 1.0000 - f1_m: 0.9981 - precision_m: 1.0000 - recall_m: 0.9963 - val_loss: 0.8592 - val_acc: 0.7640 - val_f1_m: 0.7538 - val_precision_m: 0.7669 - val_recall_m: 0.7416\n",
      "Epoch 13/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.0400 - acc: 0.9981 - f1_m: 0.9990 - precision_m: 1.0000 - recall_m: 0.9981 - val_loss: 0.8611 - val_acc: 0.7640 - val_f1_m: 0.7675 - val_precision_m: 0.7840 - val_recall_m: 0.7528\n",
      "Epoch 14/15\n",
      "534/534 [==============================] - 3s 6ms/step - loss: 0.0326 - acc: 0.9981 - f1_m: 0.9990 - precision_m: 1.0000 - recall_m: 0.9981 - val_loss: 0.8740 - val_acc: 0.7640 - val_f1_m: 0.7654 - val_precision_m: 0.7790 - val_recall_m: 0.7528\n",
      "Epoch 15/15\n",
      "534/534 [==============================] - 3s 7ms/step - loss: 0.0292 - acc: 1.0000 - f1_m: 0.9990 - precision_m: 1.0000 - recall_m: 0.9981 - val_loss: 0.8654 - val_acc: 0.7697 - val_f1_m: 0.7698 - val_precision_m: 0.7827 - val_recall_m: 0.7584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x199d5bafc18>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight={0:1,1:1,2:10,3:20}\n",
    "\n",
    "model_cnn.fit([X_train,X_char_train], y_train_cat, validation_data=([X_val,X_char_val], y_val_cat), epochs = 15,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.82        98\n",
      "           1       0.87      0.64      0.74        53\n",
      "           2       0.57      0.35      0.43        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       178\n",
      "   macro avg       0.54      0.48      0.50       178\n",
      "weighted avg       0.73      0.75      0.73       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\softwares\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# old result\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prob = model_cnn.predict([X_test,X_char_test])\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "\n",
    "# confusion_matrix(y_test[int(len(X_test)/2):], y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm (integrate cnn-char) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "EMBEDDING_DIM=300\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 534 samples, validate on 178 samples\n",
      "Epoch 1/15\n",
      "534/534 [==============================] - 11s 20ms/step - loss: 1.7236 - acc: 0.4363 - f1_m: 0.1841 - precision_m: 0.5213 - recall_m: 0.1386 - val_loss: 1.2835 - val_acc: 0.5730 - val_f1_m: 0.4823 - val_precision_m: 0.6458 - val_recall_m: 0.3876\n",
      "Epoch 2/15\n",
      "534/534 [==============================] - 7s 12ms/step - loss: 1.2440 - acc: 0.6648 - f1_m: 0.6176 - precision_m: 0.8002 - recall_m: 0.5075 - val_loss: 1.0198 - val_acc: 0.6517 - val_f1_m: 0.6111 - val_precision_m: 0.7079 - val_recall_m: 0.5393\n",
      "Epoch 3/15\n",
      "534/534 [==============================] - 7s 13ms/step - loss: 0.8825 - acc: 0.7622 - f1_m: 0.7357 - precision_m: 0.8662 - recall_m: 0.6442 - val_loss: 0.8866 - val_acc: 0.7247 - val_f1_m: 0.6810 - val_precision_m: 0.7973 - val_recall_m: 0.5955\n",
      "Epoch 4/15\n",
      "534/534 [==============================] - 7s 12ms/step - loss: 0.5758 - acc: 0.8539 - f1_m: 0.8388 - precision_m: 0.9135 - recall_m: 0.7772 - val_loss: 0.8185 - val_acc: 0.7640 - val_f1_m: 0.7490 - val_precision_m: 0.7904 - val_recall_m: 0.7135\n",
      "Epoch 5/15\n",
      "534/534 [==============================] - 7s 13ms/step - loss: 0.3697 - acc: 0.9157 - f1_m: 0.9059 - precision_m: 0.9473 - recall_m: 0.8689 - val_loss: 0.8148 - val_acc: 0.7022 - val_f1_m: 0.6928 - val_precision_m: 0.7479 - val_recall_m: 0.6461\n",
      "Epoch 6/15\n",
      "534/534 [==============================] - 7s 12ms/step - loss: 0.2425 - acc: 0.9307 - f1_m: 0.9360 - precision_m: 0.9685 - recall_m: 0.9064 - val_loss: 0.8633 - val_acc: 0.7191 - val_f1_m: 0.7245 - val_precision_m: 0.7859 - val_recall_m: 0.6742\n",
      "Epoch 7/15\n",
      "534/534 [==============================] - 7s 13ms/step - loss: 0.1648 - acc: 0.9644 - f1_m: 0.9674 - precision_m: 0.9844 - recall_m: 0.9513 - val_loss: 0.8589 - val_acc: 0.7472 - val_f1_m: 0.7499 - val_precision_m: 0.7714 - val_recall_m: 0.7303\n",
      "Epoch 8/15\n",
      "534/534 [==============================] - 8s 14ms/step - loss: 0.1233 - acc: 0.9700 - f1_m: 0.9752 - precision_m: 0.9885 - recall_m: 0.9625 - val_loss: 0.8572 - val_acc: 0.7472 - val_f1_m: 0.7406 - val_precision_m: 0.8002 - val_recall_m: 0.6910\n",
      "Epoch 9/15\n",
      "534/534 [==============================] - 7s 14ms/step - loss: 0.0919 - acc: 0.9850 - f1_m: 0.9867 - precision_m: 0.9963 - recall_m: 0.9775 - val_loss: 0.8173 - val_acc: 0.7247 - val_f1_m: 0.7091 - val_precision_m: 0.7794 - val_recall_m: 0.6517\n",
      "Epoch 10/15\n",
      "534/534 [==============================] - 8s 15ms/step - loss: 0.0668 - acc: 0.9888 - f1_m: 0.9905 - precision_m: 0.9943 - recall_m: 0.9869 - val_loss: 0.8459 - val_acc: 0.7697 - val_f1_m: 0.7626 - val_precision_m: 0.7930 - val_recall_m: 0.7360\n",
      "Epoch 11/15\n",
      "534/534 [==============================] - 9s 16ms/step - loss: 0.0435 - acc: 0.9925 - f1_m: 0.9933 - precision_m: 0.9981 - recall_m: 0.9888 - val_loss: 0.8725 - val_acc: 0.7697 - val_f1_m: 0.7606 - val_precision_m: 0.7757 - val_recall_m: 0.7472\n",
      "Epoch 12/15\n",
      "534/534 [==============================] - 7s 13ms/step - loss: 0.0323 - acc: 0.9981 - f1_m: 0.9962 - precision_m: 0.9981 - recall_m: 0.9944 - val_loss: 1.0160 - val_acc: 0.7640 - val_f1_m: 0.7628 - val_precision_m: 0.7804 - val_recall_m: 0.7472\n",
      "Epoch 13/15\n",
      "534/534 [==============================] - 7s 14ms/step - loss: 0.0218 - acc: 1.0000 - f1_m: 0.9990 - precision_m: 1.0000 - recall_m: 0.9981 - val_loss: 0.9836 - val_acc: 0.7640 - val_f1_m: 0.7558 - val_precision_m: 0.7654 - val_recall_m: 0.7472\n",
      "Epoch 14/15\n",
      "534/534 [==============================] - 8s 15ms/step - loss: 0.0172 - acc: 1.0000 - f1_m: 1.0000 - precision_m: 1.0000 - recall_m: 1.0000 - val_loss: 1.0296 - val_acc: 0.7584 - val_f1_m: 0.7539 - val_precision_m: 0.7677 - val_recall_m: 0.7416\n",
      "Epoch 15/15\n",
      "534/534 [==============================] - 8s 15ms/step - loss: 0.0143 - acc: 1.0000 - f1_m: 0.9990 - precision_m: 1.0000 - recall_m: 0.9981 - val_loss: 1.0772 - val_acc: 0.7640 - val_f1_m: 0.7524 - val_precision_m: 0.7643 - val_recall_m: 0.7416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26c096fdf98>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# part1- word embeddings\n",
    "word_input = Input(shape=(100,))\n",
    "emb=Embedding(vocabulary_size, 300, input_length=100, weights=[embedding_matrix], trainable=False)(word_input)\n",
    "hidden1 = LSTM(100)(emb)\n",
    "dropout=Dropout(0.2)(hidden1)\n",
    "\n",
    "\n",
    "# part2- character embeddings\n",
    "char_input=Input(shape=(402,300),dtype='float32')\n",
    "char_cnn1 = Convolution1D(128, 3, padding='same', strides = 2, activation='relu')(char_input)\n",
    "char_cnn1 = MaxPool1D(pool_size=5)(char_cnn1)\n",
    "char_cnn2 = Convolution1D(128, 4, padding='same', strides = 2, activation='relu')(char_input)\n",
    "char_cnn2 = MaxPool1D(pool_size=5)(char_cnn2)\n",
    "# char_cnn3 = Convolution1D(128, 5, padding='same', strides = 2, activation='relu')(char_input)\n",
    "# char_cnn3 = MaxPool1D(pool_size=5)(char_cnn3)\n",
    "\n",
    "\n",
    "#part3- concat all\n",
    "\n",
    "char_cnn = concatenate([char_cnn1,char_cnn2], axis=-1)\n",
    "flat_2 = Flatten()(char_cnn)\n",
    "flat = concatenate([dropout,flat_2], axis=-1)\n",
    "drop = Dropout(0.2)(flat)\n",
    "\n",
    "output = Dense(4, activation='softmax')(flat)\n",
    "char_lstm = Model(inputs=[word_input,char_input], outputs=output)\n",
    "char_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "# summarize layers\n",
    "# print(char_lstm.summary())\n",
    "# plot graph\n",
    "class_weight={0:1,1:1,2:1,3:10}\n",
    "\n",
    "\n",
    "char_lstm.fit([X_train,X_char_train], y_train_cat, validation_data=([X_val,X_char_val], y_val_cat), epochs = 15,class_weight=class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83        98\n",
      "           1       0.77      0.70      0.73        53\n",
      "           2       0.86      0.52      0.65        23\n",
      "           3       0.00      0.00      0.00         4\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       178\n",
      "   macro avg       0.60      0.53      0.55       178\n",
      "weighted avg       0.76      0.77      0.76       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prob = char_lstm.predict([X_test,X_char_test])\n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# can use macro avg as evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm (integrate rnn-char) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# result table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## old result of binary version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f1 score of positive class \n",
    "(the class with fewer samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|   type  | df_alzheimer | df_cancer | df_depression | df_heart_attack |df_parkinson | df_stroke  |\n",
    "| -------------| ------------- |------------- |------------- |------------- |------------- |------------- |\n",
    "| MLP  |  0.29  | 0.23  | 0.48  | 0.17  | 0.10  |0.14   |\n",
    "| LSTM  |  0.61  | 0.39  | 0.52  | 0.48  | 0.53  |0.42  |\n",
    "| CNN  |  0.42  | 0.32  | 0.48 | 0.27  | 0.14  |0.24  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### macro average of f1 score\n",
    "why choose macro average : [csdn](https://www.cnblogs.com/yuuken/p/8822496.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|   type  |  df_alzheimer | df_cancer | df_depression | df_heart_attack |df_parkinson |df_stroke  |\n",
    "| -------------| ------------- |------------- |------------- |------------- |------------- |------------- |\n",
    "| MLP  |  0.58  | 0.54  | 0.55  | 0.54  | 0.50  |0.50   |\n",
    "| LSTM  | 0.77  | 0.62  | 0.62  | 0.71  | 0.74 | 0.67  |\n",
    "| CNN  | 0.66  | 0.60 | 0.59  | 0.60 | 0.54 |0.58  | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new result of 4 classes\n",
    "macro average of f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|   type  |  df_alzheimer | df_cancer | df_depression | df_heart_attack |df_parkinson |df_stroke  |\n",
    "| -------------| ------------- |------------- |------------- |------------- |------------- |------------- |\n",
    "| MLP  |  0.38  |  0.40  | 0.35  | 0.60  | 0.69  |0.56   |\n",
    "| LSTM  | 0.37  |  0.48  | 0.41  | 0.58  | 0.75 | 0.58  |\n",
    "| Char-LSTM| 0.37    |   0.49   |   0.40   |  0.50    |  0.73    |   0.57   | \n",
    "| CNN  | 0.38  |  0.41 | 0.42  | 0.46 | 0.62 |0.47  | \n",
    "| Char-CNN| 0.45|  0.45|  0.40|   0.46|   0.66| 0.50|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some notes in training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since it's a highly imbalanced dataset in all diseases, we change the loss function by assigning weights to the different classes. For classes with very few samples, we set a relatively high weigh (5-10 times compared to the classes with many samples) \n",
    "    - Please refer to [csdn blog](https://blog.csdn.net/notHeadache/article/details/82183503) \"带权重的交叉熵loss\" 那一节。\n",
    "    - or refer to [medium](https://towardsdatascience.com/practical-tips-for-class-imbalance-in-binary-classification-6ee29bcdb8a7)-part 4. Class weighted / cost sensitive learning\n",
    "\n",
    "\n",
    "-  In the original experiments, *self-mention* and *other-mention* labels are taken as **positive class**; and *awareness* and *non-health* labels are taken as **negative class**. The same in this notebook.\n",
    "\n",
    "\n",
    "- train , val ,test split:  6 : 2 : 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- char-cnn gives some improvement compared to cnn.\n",
    "- char-lstm(integrate char-cnn module) worsen the result a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.521px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
